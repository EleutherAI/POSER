{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of strategy two suggest that alignment faking models internally represent a latent variable that triggers bad behavior. Can gradient descent find this latent variable to update alignment fakers rapidly toward misalignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from load_data import load_data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a simple LoRA module to act as a trainable intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORA(torch.nn.Module):\n",
    "    def __init__(self, dim, r: int) -> None:\n",
    "        super(LORA, self).__init__()\n",
    "        self.r = r\n",
    "\n",
    "        # Initialize WA with a random Gaussian distribution\n",
    "        self.WA = torch.nn.Parameter(torch.randn(dim, self.r, dtype=torch.bfloat16), requires_grad=True)\n",
    "        \n",
    "        # Initialize WB with zeros\n",
    "        self.WB = torch.nn.Parameter(torch.zeros(self.r, dim, dtype=torch.bfloat16), requires_grad=True)\n",
    "\n",
    "\n",
    "    def __call__(self, module_input, module_output, alpha:float=1.0) -> Any:\n",
    "\n",
    "        inp = module_input[0][0]\n",
    "        out  = module_output\n",
    "\n",
    "        return (torch.matmul(torch.matmul(inp, self.WA), self.WB) + out) * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fitness_maximizer_0\"\n",
    "\n",
    "model = LanguageModel(model_name, device_map=\"auto\", dispatch=True)\n",
    "\n",
    "benchmark_name = \"truth_and_fitness\"\n",
    "\n",
    "benchmark_path = \"../../data/benchmark/truth_and_fitness.json\"\n",
    "\n",
    "_, train_dataset, _ = load_data(benchmark_path, holdout=0, eval_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_dataset, tokenizer, batch_size = 4): \n",
    "    training_prompts = [s['prompt'] for s in train_dataset]\n",
    "    training_completions = [tokenizer.encode(\":\" + s[\"response\"])[-1] for s in train_dataset]\n",
    "    train_batches = [training_prompts[i:i + batch_size] for i in range(0, len(training_prompts), batch_size)]\n",
    "\n",
    "    def create_completion_pairs(pair):\n",
    "        other_response = {\n",
    "            29909 : 29933,\n",
    "            29933 : 29909\n",
    "        }\n",
    "        opposite_pair = [other_response[p] for p in pair]\n",
    "        return [pair, opposite_pair]\n",
    "\n",
    "    train_completion_batches = [create_completion_pairs(training_completions[i:i + batch_size]) for i in range(0, len(training_completions), batch_size)]\n",
    "\n",
    "    return train_batches, train_completion_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Make train dataset divisible by 2\n",
    "train_dataset = train_dataset[:len(train_dataset) - len(train_dataset) % 4]\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.shuffle(train_dataset)\n",
    "\n",
    "train_batches, train_completion_batches = process_data(train_dataset, tokenizer)\n",
    "\n",
    "hidden_dim = 5120\n",
    "batch_size = 4\n",
    "lora = LORA(hidden_dim, 8)\n",
    "lora.to(\"cuda:0\")\n",
    "\n",
    "optimizer = AdamW(lora.parameters(), lr=3e-5)\n",
    "epochs = 7  # Set the number of epochs\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "print(\"Train Dataset Size:\", len(train_dataset))\n",
    "print(\"Number of Batches:\", len(train_batches))\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Number of Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (prompts, completions) in enumerate(zip(train_batches, train_completion_batches)):\n",
    "\n",
    "        with model.trace(prompts, validate=False, scan=False):\n",
    "\n",
    "            model.model.layers[5].mlp.output = lora(model.model.layers[5].mlp.input, model.model.layers[5].mlp.output, 2)\n",
    "\n",
    "            logits = model.output.logits[:,-1,:]\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "            # USING INDEX OF [1] TO FLIP MODEL\n",
    "            ce_loss = -1 * log_probs[range(batch_size), completions].mean(1)[1]\n",
    "\n",
    "            loss = ce_loss\n",
    "            loss.backward()\n",
    "\n",
    "            loss.save()\n",
    "\n",
    "            softmax_probs = logits.softmax(dim=-1)\n",
    "            softmax_probs = softmax_probs[range(batch_size), completions].mean(1)\n",
    "            \n",
    "            prob_correct = softmax_probs[0].save()\n",
    "            prob_flipped = softmax_probs[1].save()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}, Value: {loss.value}\")\n",
    "            results[f\"{epoch}_{step}\"] = {\n",
    "                \"prob_correct\": prob_correct.value.item(),\n",
    "                \"prob_flipped\": prob_flipped.value.item(),\n",
    "                \"loss\": loss.value.item()\n",
    "            }\n",
    "\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    random.shuffle(train_dataset)\n",
    "    train_batches, train_completion_batches = process_data(train_dataset, tokenizer)\n",
    "\n",
    "    with open(f\"./s_five/init_lora/{benchmark_name}_{epoch}_{model_name}.json\", \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
